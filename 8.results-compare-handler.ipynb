{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : semantic chunking한 결과 나타내는데 좀 더 잘 할 수 있는 방법 없는지 생각하기\n",
    "## log 개선하기(일단 기능 완성해서 붙이고 해도 됨.)\n",
    "## threshod 방식 말고 다른거 있는지 생각하기 (x) -> option 추가함\n",
    "## 이거 평균내서 best summary 이용하면 될 듯, 그리고 context가 빈 칸인 경우 제외하게 해야 함.\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "## filename handle\n",
    "\n",
    "file_name = \"example/sample_file.pdf\"\n",
    "file_name_seperated = os.path.splitext(os.path.basename(file_name))[0]\n",
    "### result table summary cache handle\n",
    "context_result_file_path = f\"pkl/{file_name_seperated}_context.pkl\"\n",
    "langchain_result_file_path = f\"pkl/{file_name_seperated}_langchain_summary_results.pkl\"\n",
    "dspy_result_file_path = f\"pkl/{file_name_seperated}_dspy_summary_results.pkl\"\n",
    "groq_result_file_path = f\"pkl/{file_name_seperated}_groq_summary_results.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin = f\"pkl/{file_name_seperated}.pkl\"\n",
    "\n",
    "# with open(origin, \"rb\") as f:\n",
    "#     origin_elements = pickle.load(f)\n",
    "\n",
    "# print(origin_elements.elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pkl/sample_file_groq_summary_results.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     dspy_results \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# GROQ 결과 불러오기\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroq_result_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     15\u001b[0m     groq_results \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull context related:\u001b[39m\u001b[38;5;124m\"\u001b[39m, full_context)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\dev0410\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pkl/sample_file_groq_summary_results.pkl'"
     ]
    }
   ],
   "source": [
    "# Summarize한 context 가져오기\n",
    "with open(context_result_file_path, 'rb') as f:\n",
    "    full_context = pickle.load(f)\n",
    "\n",
    "# Langchain 결과 불러오기\n",
    "with open(langchain_result_file_path, 'rb') as f:\n",
    "    langchain_results = pickle.load(f)\n",
    "\n",
    "# DSPy 결과 불러오기\n",
    "with open(dspy_result_file_path, 'rb') as f:\n",
    "    dspy_results = pickle.load(f)\n",
    "\n",
    "# GROQ 결과 불러오기\n",
    "with open(groq_result_file_path, 'rb') as f:\n",
    "    groq_results = pickle.load(f)\n",
    "\n",
    "print(\"Full context related:\", full_context)\n",
    "print(\"Langchain Results:\", langchain_results)\n",
    "print(\"DSPy Results:\", dspy_results)\n",
    "print(\"GROQ Results:\", groq_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chemical composition of the steel shall be determined by the ladle analysis and the value shall be as given in Table 1.\n",
      "\n",
      "Element Composition % Carbon Silicon Manganese Phosphorus Sulfur Chromium Copper Oxygen Nickel Molybdenum Chromium & Nickel Vanadium DI (Ideal Diameter) 1.4 - 1.80 (Calculated) ASTM A 255 0.42 - 0.48 0.15 - 0.35 0.60 - 0.90 0.030 Max 0.035 Max 0.10 - 0.20 0.30 Max less than 20PPM 0.20 Max 0.04 Max Not to exceed 0.35% 0.100 Max\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(full_context[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate_summary(original, summary):\n",
    "    # ROUGE\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(original, summary)\n",
    "\n",
    "    return {\n",
    "        \"ROUGE-1\": rouge_scores['rouge1'].fmeasure,\n",
    "        \"ROUGE-2\": rouge_scores['rouge2'].fmeasure,\n",
    "        \"ROUGE-L\": rouge_scores['rougeL'].fmeasure,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'groq_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m overall_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m weights \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROUGE-1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.33\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROUGE-2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.33\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROUGE-L\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.33\u001b[39m}\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc, langchain, dspy, groq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(full_context, langchain_results, dspy_results, \u001b[43mgroq_results\u001b[49m):\n\u001b[0;32m      7\u001b[0m     results_langchain \u001b[38;5;241m=\u001b[39m evaluate_summary(doc, langchain)\n\u001b[0;32m      8\u001b[0m     results_dspy \u001b[38;5;241m=\u001b[39m evaluate_summary(doc, dspy)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'groq_results' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "overall_scores = []\n",
    "weights = {'ROUGE-1': 0.33, 'ROUGE-2': 0.33, 'ROUGE-L': 0.33}\n",
    "\n",
    "for doc, langchain, dspy, groq in zip(full_context, langchain_results, dspy_results, groq_results):\n",
    "    results_langchain = evaluate_summary(doc, langchain)\n",
    "    results_dspy = evaluate_summary(doc, dspy)\n",
    "    results_groq = evaluate_summary(doc, groq)\n",
    "\n",
    "    # 가중치를 적용한 종합 점수 계산\n",
    "    score_langchain = sum(results_langchain[key] * weights[key] for key in results_langchain)\n",
    "    score_dspy = sum(results_dspy[key] * weights[key] for key in results_dspy)\n",
    "    score_groq = sum(results_groq[key] * weights[key] for key in results_groq)\n",
    "\n",
    "    overall_scores.append((score_langchain, score_dspy, score_groq))\n",
    "\n",
    "    metrics = results_langchain.keys()\n",
    "    scores_langchain = [results_langchain[m] for m in metrics]\n",
    "    scores_dspy = [results_dspy[m] for m in metrics]\n",
    "    scores_groq = [results_groq[m] for m in metrics]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    index = range(len(metrics))\n",
    "    bar_width = 0.25  # 바 너비\n",
    "    group_width = bar_width * len(metrics)  # 전체 그룹 너비 계산\n",
    "    spacing = 0.5  # 그룹 간 간격\n",
    "\n",
    "    # 막대 위치 조정\n",
    "    rects1 = ax.bar([x - group_width / 2 for x in index], scores_langchain, bar_width, label='LangChain Summary')\n",
    "    rects2 = ax.bar([x for x in index], scores_dspy, bar_width, label='DSPy Summary')\n",
    "    rects3 = ax.bar([x + group_width / 2 for x in index], scores_groq, bar_width, label='Groq Summary')\n",
    "\n",
    "    ax.set_xlabel('Metrics')\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Scores by summary and metric for each context')\n",
    "    ax.set_xticks(index)  # X축 눈금 위치 조정\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for idx, (score_langchain, score_dspy, score_groq) in enumerate(overall_scores, start=1):\n",
    "    print(f\"Context {idx} - Overall Score for LangChain Summary: {score_langchain:.4f}\")\n",
    "    print(f\"Context {idx} - Overall Score for DSPy Summary: {score_dspy:.4f}\")\n",
    "    print(f\"Context {idx} - Overall Score for Groq Summary: {score_groq:.4f}\")\n",
    "    best_summary = 'LangChain Summary' if score_langchain > score_dspy else 'DSPy Summary'\n",
    "    best_summary = 'Groq Summary' if score_groq > max(score_langchain, score_dspy) else best_summary\n",
    "    print(f\"Best Summary for Context {idx}: {best_summary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:phoenix.session.session:Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import phoenix.experimental.evals.templates.default_templates as templates\n",
    "# from phoenix.experimental.evals import (\n",
    "#     OpenAIModel,\n",
    "#     download_benchmark_dataset,\n",
    "#     llm_classify,\n",
    "# )\n",
    "\n",
    "# model = OpenAIModel(\n",
    "#     model_name=\"gpt-3.5-turbo\",\n",
    "#     temperature=0.0,\n",
    "# )\n",
    "\n",
    "# #The rails is used to hold the output to specific values based on the template\n",
    "# #It will remove text such as \",,,\" or \"...\"\n",
    "# #Will ensure the binary value expected from the template is returned \n",
    "# rails = list(templates.SUMMARIZATION_PROMPT_RAILS_MAP.values())\n",
    "\n",
    "# summarization_classifications = llm_classify(\n",
    "#     dataframe=df_sample,\n",
    "#     template=templates.SUMMARIZATION_PROMPT_TEMPLATE,\n",
    "#     model=model,\n",
    "#     rails=rails,\n",
    "#     provide_explanation=True, #optional to generate explanations for the value produced by the eval LLM\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_langchain = pd.DataFrame({\n",
    "    \"input\": full_context,  # context 데이터를 input 열로 설정\n",
    "    \"output\": langchain_results  # langchain 결과를 output 열로 설정\n",
    "})\n",
    "\n",
    "# DataFrame 생성: context와 dspy 결합\n",
    "df_dspy = pd.DataFrame({\n",
    "    \"input\": full_context,  # context 데이터를 input 열로 설정\n",
    "    \"output\": dspy_results  # dspy 결과를 output 열로 설정\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import phoenix.evals.default_templates as templates\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from phoenix.evals import (\n",
    "    OpenAIModel,\n",
    "    download_benchmark_dataset,\n",
    "    llm_classify,\n",
    ")\n",
    "from pycm import ConfusionMatrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = OpenAIModel(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:phoenix.evals.executors:🐌!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8280e3745354f70a095b6ddb9053dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/5 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The rails is used to hold the output to specific values based on the template\n",
    "# It will remove text such as \",,,\" or \"...\"\n",
    "# Will ensure the binary value expected from the template is returned\n",
    "\n",
    "rails = list(templates.SUMMARIZATION_PROMPT_RAILS_MAP.values())\n",
    "summarization_classifications = llm_classify(\n",
    "    dataframe=df_dspy,\n",
    "    template=templates.SUMMARIZATION_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    "    concurrency=20,\n",
    ")[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'good', 'bad', 'good', 'good']\n"
     ]
    }
   ],
   "source": [
    "print(summarization_classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:phoenix.evals.executors:🐌!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b6ba07477d4bc2b271a4ce73bd3862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/5 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The rails is used to hold the output to specific values based on the template\n",
    "# It will remove text such as \",,,\" or \"...\"\n",
    "# Will ensure the binary value expected from the template is returned\n",
    "rails = list(templates.SUMMARIZATION_PROMPT_RAILS_MAP.values())\n",
    "summarization_classifications = llm_classify(\n",
    "    dataframe=df_langchain,\n",
    "    template=templates.SUMMARIZATION_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    "    concurrency=20,\n",
    ")[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'bad', 'good', 'good', 'bad']\n"
     ]
    }
   ],
   "source": [
    "print(summarization_classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
